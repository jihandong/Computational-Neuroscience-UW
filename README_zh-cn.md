# Computational-Neuroscience-UW

在近几年AI的研究成果的面前，这个课程已经过时了，尤其是对那些已经知道很多神经网络知识的人。但是从这些课程中仍然能见识到过去的神经科学家是如何分析大脑的。

Module 1: Introduction. 介绍了神经科学的一些生化背景知识，只比高中生物深入一点点，只要了解神经元结构就足够了，大脑结构的知识虽然很有趣但是并没有在这个课程中涉及太多。

Module 2: Neuron Encoding. 信号与系统中卷积的应用，把函数视作无穷维向量的方法很有趣（希尔伯特空间？）。

Module 3: Neuron Decoding. 应用了统计学中的贝叶斯方法和高斯噪声。

Module 4: Information Theory. 应用了熵来量化神经元的编码效率和信息，KL散度是个比较随机变量的有用工具。

Module 5: Ion Channel Models. 应用了微分方程来模拟神经元的电流变化（与 RC 回路类似），以及神经元的电流变化对神经元的兴奋程度的影响；其中 Plane Analysis 的数学工具对 CS 背景的学生来说会比较陌生。

Module 6: Networks. 应用了随机过程来分析神经网络的连接结构，以及网络的稳定性，但可惜没有详细和正式地介绍这些方法。

Module 7: Neuron Plasiticity. 引入了 Hebbian & Oja 等学习规则来模拟突触的自主学习，并且介绍了梯度下降法的应用，前者相比后者应该是比较原始但是启发性的方法。

Module 8: Supervisions & Rewards. 谈到了前馈神经网络和强化学习，总算有了一点现代的东西，但可惜这就是课程的结尾了，对一门将近10年前的课程也很难要求太多。

