# Infomation Theory in Neuroscience

## Basic Knowledge

**Entropy**: measure the quantity of *uncertainty*, the amount of infomation.

$$
e = -\sum_i p_i log_2 p_i
$$

**Mutual Infomation**:

$$
\begin{align*}
I(S;R)
&= H(S) + H(R) - H(SR) \\
&= H(S) - H(S|R) \\
&= H(R) - H(R|S) \\
&= D_{KL}[P(R,S), P(R)*P(S)]
\end{align*}
$$

> Be aware of the Symmetry of Multual Info, sometimes *one is easier to calculate than another one*.

## In Neuroscience

We regard $H(R|S)$ as **noise entropy**, because it excludes the influence of stimulus, so $H(R|S)$ only represent *the uncertainty of response itself, also the noises generated by cell itself*.

And more insight:

$$
\begin{align*}
H(S) &: \text{Theoritical all Infomations of Stimulus} \\
H(S|R) &: \text{Infomations of Stimulus told by Response} \\
H(R) &: \text{Everything u might heard from Response} \\
H(R|S) &: \text{Something u heard from Response after Stimulus}
\end{align*}
$$

Easier formula to calculate the Mutual Entropy:

$$
I(S;R) = H(R) - E_s(H(R|S=s))
$$
